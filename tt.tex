This chapter shall serve to introduce the basic notions of type theory which
we will need for the subsequent content of this thesis.
At first (Chapter~\ref{sec:tt-dtt}), we will have a general look on dependent type theory, its use and
how it differs from a set theoretic foundation,
afterwards (Chapter~\ref{sec:tt-w}) we want to unify the notion of an inductive
type using the concept of indexed W-types.
Then (Chapter~\ref{sec:tt-hott}), we will explain how homotopy type theory was created to have a suitable
language to reason about higher equalities and provides a synthetic way to
formalize topological insights.
Finally, in Chapter~\ref{sec:tt-provers}, we will give examples of two theorem
provers based on dependent types, Agda and Lean, and point out some of their differences.

\section{Dependent Type Theory}\label{sec:tt-dtt}

While the term ``type theory'' stems from the early nineteenth centry, when
Bertrand Russell sought to lay out an alternative to set theory which did
suffer from the paradox which Russell discovered,
todays versions of type theory have little in common with Russell's attempts
but rather rely on the considerations of Per Martin-Löf (\cite{martin-lof2, martin-lof1})
who, starting in the 1970's, built a new mathematical foundation based on the
$\lambda$-calculus, himself drawing inspiration from previous logicians and
mathematicians like Alonzo Church and Haskell Curry.
Often, dependent type theory is also referred to as Martin-Löf type theory.

Based on a phenomenon knows as the ``Curry-Howard correspondence'', type theory
can serve both as a theoretical foundation for a formal representation of mathematics,
as well as the principle for  the specification for strongly typed functional
programming languages.
It was implemented in computer languages for programming and theorem proving which
are used massively in the field of formal verification and in the formalization of
mathematics.
Among the most commonly used implementations are the theorem prover Coq (\cite{coq})
which notably has a lot of users in the field of hardware verification,
the prover Agda (\cite{agda}), which is popular amongst type theorists themselves,
and the Microsoft Research based project Lean(\cite{mouracade}), which has
drawn considerable attention from researching mathematicians as a tool to
formally verify their proofs.

Type theory differs from a set theoretic mathematical foundation (let us, as a
point of reference, consider set theories based on (higher-order) propositional logic
like Zermelo-Fraenkel set theory) in several
important aspects:
\begin{itemize}
\item Type theory follows a paradigm called \emph{``propositions-as-types''}.
This means that statements like theorems and conjectures are represented
using the same class of objects as other data like sets or (algebraic) structures.
In contrast to this, most set theoretic foundations are built on a \emph{dichotomy between
the propositions and the objects} they describe:
They first start out with a logical frame work on which axiomatically a theory
of sets is introduced.
The coherence between these two levels must then be created using an axiom like
the excision axiom in Zermelo-Fraenkel set theory.
\item Type theory is \emph{typed} while set theory is \emph{untyped}.
While in set theory, objects can be an element of different sets --
consider the number two which is an element both of the set of even integers
as well as the set of all integers --
type theory is based on the principle that every piece of data (every term)
is assigned a unique type which is known at the point of the creation of the data.
Since this assignment, called typing, is decidable, we consider it a judgment
rather than a provable proposition that a given term $t$ has type $A$.
\item Type theory is inherently \emph{constructive} while most of the most
well-known set theoretic foundations are \emph{non-construcitve}.
This has great consequences for the computational use of represented mathematics:
Every type theoretic function the domain of which are the natural numbers, can compute a
numeral for any given input.
\item While in set theory, \emph{sets ar the only primitives}, type theory
provides often-used objects like \emph{functions and inductively defined types
as primitives}.
This means that we can use these without having to care about how to assemble them
from other primitives. 
It also offers modularity in the follwowing sense: %TODO find a better word than modularity
In constrast to a function in set theory, a function in type theory only reveals
its constituing information:
What is the resulting output for a given input?
\end{itemize} %TODO maybe say something about sizes

Let us now fix some notation and some basic constructions which will occur
throughout this text.
As mentioned, we will, whenever we talk about a certain piece of type theoretic
data, accompany it with its type,
we need a notation for this kind of \textbf{type judgment}:
We write $t : A$ to state that the term $t$ is of type $A$.
This is similar to elements in a set, but it also represents the fact
that $t$ is a proof for a proposition $A$.
Sometimes, we want to express that two terms $s$ and $t$ of the same type $A$
only differ an unfolding or folding of a definition, or that the application of
a reduction rule to $s$ results in $t$.
We will notate this by $s \equiv t$.
Our type theory will then make no distinction between $s$ and $t$.
This means that if $A$ and $B$ are types with $A \equiv B$, then
$s : A$ implies $s : B$.
To keep type checking decidable it is easy to see that it is important
to keep the question whether two terms are what we call \textbf{definitionally equal}
decidable itself.

We said above that each piece of data, so each term,
has a unique type (up to definitional equality).
This statement also holds true for types itself.
Types which themselves contain types, are called \textbf{universes} and we will
denote them using $\UU$.
The universe itself also needs a type, but
assuming $\UU : \UU$ is inconsistent (this is often referred to as
``Girard's paradox'' (\cite{girard72,hurkens95}) which can be seen as the type theoretic equivalent
to Russell's paradox).
So the solution we apply for the remainder of this text is
to assume that we have an infinte chain of universes
\begin{equation*}
\UU_0 : \UU_1,~~ \UU_1 : \UU_2,~~ \UU_2 : \UU_3,~~ \ldots \text{,}
\end{equation*}
each contained in the next one.
Most often, we will chose to leave the index implicit and regard our constructions
as being \emph{universe polymorphic}, meaning that they are valid in any chosen
universe.
Some type theories are constructed to be \emph{cumulative} in the sense that whenever
we have a type $A : \UU_i$, it is a type in the succeeding universe, so $A : \UU_{i+1}$.
Our constructions will not rely on cumulativity and they are formalized in
non-cumulative type theories.

Until now, we have not talked about any way to form types.
In the following we will get to know the non-dependent version of some basic types,
some of which will later be generalized to a dependent form.
These type formers will be presented in a very similar way:
We will give a rule on how to form the type itself (called formation rule),
rules on how to construct elements of the type (called introduction rules),
rules on how to use the elements of the type (called elimination rules),
and some of them will be followed by some reduction rules offering definition
equalities used to simplify terms.
As we have mentioned in the comparison to set theory, functions are basic building
blocks in type theory.
The non-dependent functions form what is known as \emph{simply typed $\lambda$-calculus}.
The formation, introduction, and elimination rules correspond to the fact that
we can form the function type of any type for its domain and any type for its
codomain, the fact that we can build a function by
specifying its output for any given input, and the fact that we can apply
a function to any term of its domain:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\to$-Form]{A,B : \UU}{A \to B : \UU} \qquad
\inferrule*[left=$\to$-Intro]{a : A \vdash \Phi[a/x] : B}{(\lambda x. \Phi) : A \to B} \\[.7em]
\inferrule*[left=$\to$-Elim]{f : A \to B \\ a : A}{f(a) : B}
\end{gathered}
\end{equation*}
Here, $\Phi$ is a term that may have $x$ as a free variable.
$\Phi[a/x]$ denotes the replacement by every appearance of $x$ by $a$.
Additional to these rules we also have $\eta$-conversion and $\beta$-reduction:
\begin{equation*} \label{eq:eta-beta}
\begin{aligned}
\inferrule*[left=$\eta$]{f : A \to B}{(\lambda x. f(x)) \equiv f} \qquad
\inferrule*[left=$\beta$]{(\lambda x. \Phi) : A \to B \\ a : A}
	{(\lambda x. \Phi)(a) \equiv \Phi[a/x]}
\end{aligned}
\end{equation*}
In type theory, these functions are used to represent
both functions between sets as well as implications between propositions.
A special kind of function are those of the form $A \to \UU$.
These so called \textbf{type families} are used to represent set-valued
functions as well as propositions with a free variable in $A$.

\begin{remark}\label{rmk:tt-syntax}
When giving inference rules as the ones above, we always assume an arbitray
\emph{context} of variables for their premises as well as their conclusion.
Often, when presenting the syntax of a type theory, rules on how
to form these contexts $\vdash \Gamma$ are given together with rules for the
formation of types $\Gamma \vdash A$ in a given context $\Gamma$ and the
treatment of terms $\Gamma \vdash t : A$ of a given type $A$ in context $\Gamma$.
We will choose to be immplicit about the variable context in this chapter, while
being more explicit about them when we introduce other syntaxes in later chapter,
for which a more formal treatment is appropriate.
\end{remark}

Since we want a representation of logics in type theory, and so far the only logical
connective we introduced are implications, we might next ask for types representing
the propositions ``true'' and ``false''.
Since the elements of this type should correspond to the proofs of ``true'' and
``false'', we can conclude that the type corresponding to ``true'' should contain
one canonical element, while the one corresponding to ``false'' should contain
none.
Thus, we will call these two types the \textbf{empty type} and the \textbf{unit type},
respectively and denote them with $\emptytype$ and $\unit$.
The formation and introduction rules for these are not very surprising:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\emptytype$-Form]{ }{\emptytype : \UU} \qquad
\inferrule*[left=$\unit$-Form]{ }{\unit : \UU} \qquad
\inferrule*[left=$\unit$-Intro]{ }{\star : \unit} \\[.7em]
\end{gathered}
\end{equation*}
The elimination rule for $\emptytype$ says the we can derive a proof for any statement,
given a proof of ``false'' (``ex falso quodlibet''), while the elimination rule
for $\unit$ specifies that when showing a statement about the elements of the
unit type, it suffices to consider its canonical element $\star$:
\begin{equation*}
\inferrule*[left=$\emptytype$-Elim]{C : \emptytype \to \UU \\ x : \emptytype}
	{\elim_\emptytype(C,x) : C(x)} \qquad
\inferrule*[left=$\unit$-Elim]{C : \unit \to \UU \\ p : C(\star) \\ x : \unit}
	{\elim_\unit(C,p,x) : C(x)}
\end{equation*}
with the reduction rule $\elim_\unit(C,p,\star) \equiv p$.

To give an example for a type which contains more than just one element, we can
consider the type of booleans $\twotype$, containing elements $0$ and $1$.
This time, we have more than just one constructor, and the elimination rule
requires us to give proofs for both of the two elements:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\twotype$-Form]{ }{\twotype : \UU} \qquad
\inferrule*[left=$\twotype$-Intro1]{ }{0_\twotype : \twotype} \qquad
\inferrule*[left=$\twotype$-Intro2]{ }{1_\twotype : \twotype} \\[.7em]
\inferrule*[left=$\twotype$-Elim]{C : \twotype \to \UU \\ p_0 : C(0_\twotype) \\ p_1 : C(1_\twotype) \\ x : \twotype}
  {\elim_\twotype(C, p_0, p_1, x) : C(x) }
\end{gathered}
\end{equation*}
with the reduction rules $\elim_\twotype(C, p_0, p_1, 0_\twotype) \equiv p_0$
and $\elim_\twotype(C, p_0, p_1, 1_\twotype) \equiv p_1$.

As a minimal example of an infinite type, the set theoretic equivalent of which
would be introduced axiomatically, we can take a look at the type of
natural numbers.
It is generated inductively by the zero element and the successor function.
The eliminator provides the principle of induction on the natural numbers, which
also makes that we can assume that every element of the natural numbers is a
result of repeatedy applying the successor function to zero:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\N$-Form]{ }{\N : \UU} \qquad
\inferrule*[left=$\N$-Intro]{ }{0 : \UU \\ S : \N \to \N} \\[.7em]
\inferrule*[left=$\N$-Elim]
	{C : \N \to \UU \\ p : C(0) \\ q : \textstyle{\prod_{(n : \N)}} C(n) \to C(S(n)) \\
		x : \N}
	{\elim_\N(C, p, q, x) : C(x)}
\end{gathered}
\end{equation*}
with the reduction rules
\begin{align*}
\elim_\N(C, p, q, 0) &\equiv p \text{ and} \\
\elim_\N(C, p, q, S(x)) &\equiv q(x, \ind_\N(C, p, q, x)) \text{.}
\end{align*}
Note that we regain, by restricting $\elim_N$ to the \emph{non-dependent
eliminator} or \emph{recursor},
the usual way to recursively define functions $f : \N \to A$ for
a type $A : \UU$ by providing the value $f(0)$ and, for each $n : \N$ the
recursive definition $f(n) \to f(S(n))$.
It has the following type:
\begin{equation*}
\rec_\N(A) \equiv \ind(\lambda x. A) : A \to (\N \to A \to A) \to \N \to A \text{.}
\end{equation*}

So far we haven't done the word ``dependent'' in the title of this chapter
much justice,
but now we will move on to introduce some dependent type formers.
The first of these will be \textbf{$\Pi$-types} or \text{dependent function types}.
When considering non-dependent functions, the codomain was a fixed type $B$
such that for all inputs $a : A$, the output $f(a)$ is an element of $B$.
For dependent functions, however, the codomain is a type family $B : A \to \UU$,
and for each input $a : A$ the output $f(a)$ is of type $B(a)$.
Considering we want to use type families to represent propositions depending on
a free variable, these functions represent the universal quantification over this
free variable!
While for application and $\lambda$-abstraction we don't introduce new notation
for dependent functions, we will denote the type of all dependent functions
on a type family $B : A \to \UU$ as $\prod_{(a : A)} B(a)$, or, alternatively,
as $\Pi(B)$ (as a shorter variant) or $(a : A) \to B(a)$ (often called
``Agda-notation'' in referral to the theorem prover of that name).
The rules to form the type of $\Pi$-types and to introduce and apply dependent
functions generalize the rules for non-dependent functions as follows:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\Pi$-Form]{A : \UU_i \\ B : A \to \UU_j}
	{\textstyle{(\prod_{(a : A)} B(a))} : \UU_{\max\{i,j\}}} \qquad
\inferrule*[left=$\Pi$-Intro]{a : A \vdash \Phi[a/x] : B(a)}
	{(\lambda x. \Phi) : \textstyle{\prod_{(a : A)} B(a)}} \\[.7em]
\inferrule*[left=$\Pi$-Elim]{f : \textstyle{\prod_{(a : A)} B(a)} \\ a : A}
	{f(a) : B(a)}
\end{gathered}
\end{equation*}
Again, we have the rules for $\beta$-reduction and $\eta$-conversion like in
the non-dependent case, yielding judgmental equalities
$(\lambda x. f (x)) \equiv f$ and $(\lambda x. \Phi)(a) \equiv \Phi[a/x]$.
Having $\Pi$-types at our disposal allows us to state the rules governing
a couple of further essential type formers.
Note that, once we have added dependent functions, we can rediscover
non-dependent functions as the special case of dependent functions over
a constant type family.

Two important logical connectives are still missing:
Conjunction and disjunction. In type theory these coincide with
the product and disjoint union (sum) of types.
The rules for the product type are as follows:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\times$-Form]{A, B : \UU}{A \times B : \UU} \qquad
\inferrule*[left=$\times$-Intro]{a : A \\ b : B}{(a, b) : A \times B} \\[.7em]
\inferrule*[left=$\times$-Elim]{C : A \times B \to \UU \\
	p: (a : A)(b : B) \to C(a,b) \\ x : A \times B}
	{\elim_{A \times B}(C, p, x) : C(x)}
\end{gathered}
\end{equation*}
with the reduction rule $\elim_{A \times B}(C, p, (a, b)) \equiv p(a, b)$.
The projections of an instance $x : A \times B$ are then
defined by induction:
\begin{align*}
\pr_1(x) &:\equiv \elim_{A \times B}((\lambda y. A), (\lambda a. \lambda b. a), x) : A \text{ and}\\
\pr_2(x) &:\equiv \elim_{A \times B}((\lambda y. A), (\lambda a. \lambda b. b), x) : B \text{,}
\end{align*}
yielding $\pr_1((a, b)) \equiv a$ and $\pr_2((a, b)) \equiv b$ judgmentally.
The type representing disjunction has two constructors determining whether
we provide proof for its left or its right type:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$+$-Form]{A : \UU \\ B : \UU}{A + B : \UU} \\[.7em]
\inferrule*[left=$+$-Intro1]{a : A}{\inl(a) : A + B \\ \inr(b) : A + B} \\[.7em]
\inferrule*[left=$+$-Elim]
	{C : (A + B) \to \UU \\  p : (a : A) \to C(\inl(a))
		\\ q : (b : B) \to C(\inr(b)) \\ x : A + B}
	{\elim_{A + B}(C, p, q, x) : C(x)}
\end{gathered}
\end{equation*}
For the sum type, we have the reduction rules
\begin{align*}
\elim_{A + B}(C, p, q, \inl(a)) &\equiv p(a) \text{ and} \\
\elim_{A + B}(C, p, q, \inr(b)) &\equiv q(b) \text{.}
\end{align*}

Looking at the sum type, we can find a slight generalization which is very useful
when we want to model existential quantification of a type family $B : A \to \UU$
in type theory:
A version of the sum type where the type of the second component of a pair $(a, b)$
may depend on $a : A$ via the type family, i.\,e. $b : B(a)$.
The type holding this kind of pair for a fixed type family $B : A \to \UU$
is called the \textbf{dependent sum type} or \textbf{$\Sigma$-type} over $B$.
We will again three different notations for this type, of which in this text
we will mostly prefer the latter one:
The type is usually (e.\,g. in \cite{hottbook}) denoted by
$\sum_{(a : A)} B(a)$, mimicing mathematical notation for sums.
Furthermore there is the short variant of writing $\Sigma(B)$ and
an Agda-inspired notation $(a : A) \times B(a)$.
The inference rules for this type are just a slight generalization of the
rules we have already seen for the non-dependent sum type:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\Sigma$-Form]{A : \UU \\ B : A \to \UU}
	{(a : A) \times  B(a) : \UU} \qquad
\inferrule*[left=$\Sigma$-Intro]{a : A \\ b : B(a)}
	{(a, b) : (a : A) \times  B(a)} \\[.7em]
\inferrule*[left=$\Sigma$-Elim]
	{C : (a : A) \times  B(a) \to \UU \\
		p : (a : A)(b : B(a)) \to C((a,b)) \\
		x :  (a : A) \times B(a) }
	{\elim_{(a : A) \times B(a)}(C, p, x) : C(x)}
\end{gathered}
\end{equation*}

With the type formers we met so far, we can already represent a lot of mathematical
definitions and knowledge.
For example, if we wanted to define what it means for a natural number to be odd,
we could set
\begin{equation*}
\isodd :\equiv \rec_\N(\UU, \emptytype, (\lambda n. \lambda A. A \to \emptytype)) : \N \to \UU \text{.}
\end{equation*}
For example, this gives us the statement that the number one is odd, witnessed by
the following term:
\begin{align*}
(\lambda x. x) :~ & \emptytype \to \emptytype \\
 &\equiv \elim_\N((\lambda x. \UU), \emptytype, (\lambda n. \lambda A. A \to \emptytype), 0)
  \to \emptytype\\
 &\equiv \elim_\N((\lambda x. \UU), \emptytype, (\lambda n. \lambda A. A \to \emptytype), S(0))\\
 &\equiv \isodd(S(0)) \text{.}
\end{align*}

\section{Inductive Types}\label{sec:tt-w}

So far, the presented type formers may seem like a wild zoo of unrelated,
random examples.
Some are generalization of others (like $\Sigma$-types generalize sums),
but the question one might ask is if there is any overarching principle behind
the choice of those type formers.
The feature that all of them have in common is that, rather than by an enumeration
of their elements,
they are defined by their formation, introduction, and elimination rules -- their
elements are those which are generated \emph{inductively} by their introduction
rules, also called \emph{constructors}.

Some of the type formers we have seen were \emph{parameterized} by other types,
but more than that, dependently types allow us to specify \emph{indexed} inductive
types.
One example for this is the type of \emph{vectors} of a type $A$.
Vectors are a variant of lists, where we use the typing to keep track of the
length of its elements:
The type of vectors on $A$, is not a type but a type family:
\begin{equation*}
\vecty_{A} : \N \to \UU \text{.}
\end{equation*}
It has two constructors and eliminators of the following form:
\begin{equation*}
\begin{gathered}
\inferrule{ }
  {\nil : \vecty(0)}
\qquad
\inferrule{n : \N \\ a : A \\ v : \vecty(n)}
  {\cons(a, v) : \vecty(n + 1)} \\[.7em]
\inferrule{C : \{n : \N\} \to \vecty(n) \to \UU \\
  p_\nil : C(\nil) \\
  p_\cons : \{n : \N\}(a : A)(v : \vecty(n)) \to C(v) \to C(\cons(a,v)) \\
  n : \N \\ v : \vecty(n)}
  {\elim_\vecty(C, p_\nil, p_\cons, v) : C(v)}
\end{gathered}
\end{equation*}
with two reduction rules
\begin{align*}
\elim_\vecty(C, p_\nil, p_\cons, \nil) &\equiv p_\nil \text{ and} \\
\elim_\vecty(C, p_\nil, p_\cons, \cons(a, v)) &\equiv p_\cons(a, v, \elim_\vecty(C, p_\nil, p_\cons, v)) \text{.}
\end{align*}
In natural language, the eliminator says that to show a statement about vectors
it is sufficient to prove it about the empty vector and that the statement is stable
under extending an arbitrary vector.
Note that the natural number in the conclusion of both introduction rules
is not a variable, and the $\cons$ even modifies the natural number.
As such, we don't call the dependency on the natural numbers in the type of vectors
an \emph{index} instead of a parameter.

While dependent and non-dependent functions were primitives needed to even talk
about other type formers,
all other examples which we have seen, can be captured by the concept of
\emph{indexed inductive type}.
But since it is not formally strict to just assume the presence of indexed
inductive types based on giving examples,
different attempts have been made to make precise a definition of what
an indexed inductive type is.
One schematic approach was made by \cite{dybjer94}, while the approach which
we want to introduce here requires an encoding of the constructors in a very specific
way to be able to present the inductive type as an instance of a very general
form of a \emph{tree}.
The types covered by this approach are called \emph{indexed W-Types}, and they
cover a bigger fragment of inductive types than the non-indexed version of
\emph{W-Types} which were introduced by foo, and which were,
in a topos theoretic setting, discussed by \cite{moerdijkwellfounded}.
The generalization to indexed W-Types, as presented here,
was found by \cite{indexedcontainers}.

\begin{defn}
Assume that we are given the following data:
\begin{itemize}
\item A type $I : \UU$ of \emph{indices},
\item a type $A : \UU$ encoding the number and non-recursive input \emph{data} or \emph{shapes} for contructors,
\item a function $o : A \to I$ assigning to each piece of input data the corresponding \emph{output index} of
the constructor,
\item a type $B : A \to \UU$ of \emph{recursive occurences} or \emph{positions} for each
bit of input data, and
\item a function $r : (a : A) \to B(a) \to I$ of indices of \emph{recursive occurences}.
\end{itemize}
Then, we assume that we have
a type $\IW{A}{B}{o}{r} : I \to \UU$ with the following introduction and elimination
rules:
\begin{equation*}
\begin{gathered}
\inferrule*[left=IW-Intro]{a : A \\
  b : B(a) \to \IW{A}{B}{o}{r}(r(a, b))}
  {\IWsup{a}{b} : \IW{A}{B}{o}{r}(o(a))} \\[.7em]
\inferrule*[left=IW-Elim]{foo}
  {bar}
\end{gathered}
\end{equation*}
\end{defn}

\section{Homotopy Type Theory}\label{sec:tt-hott}

\section{Theorem Provers Based on Type Theory}\label{sec:tt-provers}








