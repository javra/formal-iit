This chapter shall serve to introduce the basic notions of type theory which
we will need for the subsequent content of this thesis.
At first (Chapter~\ref{sec:tt-dtt}), we will have a general look on dependent type theory, its use and
how it differs from a set theoretic foundation,
then (Chapter~\ref{sec:tt-hott}) we will explain how homotopy type theory was created to have a suitable
language to reason about higher equalities and provides a synthetic way to
formalize topological insights.
Finally, in Chapter~\ref{sec:tt-provers}, we will give examples of two theorem
provers based on dependent types, Agda and Lean, and point out some of their differences.

\section{Dependent Type Theory}\label{sec:tt-dtt}

While the term ``type theory'' stems from the early nineteenth centry, when
Bertrand Russell sought to lay out an alternative to set theory which did
suffer from the paradox which Russell discovered,
todays versions of type theory have little in common with Russell's attempts
but rather rely on the considerations of Per Martin-Löf (\cite{martin-lof2, martin-lof1})
who, starting in the 1970's, built a new mathematical foundation based on the
$\lambda$-calculus, himself drawing inspiration from previous logicians and
mathematicians like Alonzo Church and Haskell Curry.
Often, dependent type theory is also referred to as Martin-Löf type theory.

Based on a phenomenon knows as the ``Curry-Howard correspondence'', type theory
can serve both as a theoretical foundation for a formal representation of mathematics,
as well as the principle for  the specification for strongly typed functional
programming languages.
It was implemented in computer languages for programming and theorem proving which
are used massively in the field of formal verification and in the formalization of
mathematics.
Among the most commonly used implementations are the theorem prover Coq (\cite{coq})
which notably has a lot of users in the field of hardware verification,
the prover Agda (\cite{agda}), which is popular amongst type theorists themselves,
and the Microsoft Research based project Lean(\cite{mouracade}), which has
drawn considerable attention from researching mathematicians as a tool to
formally verify their proofs.

Type theory differs from a set theoretic mathematical foundation (let us, as a
point of reference, consider set theories based on (higher-order) propositional logic
like Zermelo-Fraenkel set theory) in several
important aspects:
\begin{itemize}
\item Type theory follows a paradigm called \emph{``propositions-as-types''}.
This means that statements like theorems and conjectures are represented
using the same class of objects as other data like sets or (algebraic) structures.
In contrast to this, most set theoretic foundations are built on a \emph{dichotomy between
the propositions and the objects} they describe:
They first start out with a logical frame work on which axiomatically a theory
of sets is introduced.
The coherence between these two levels must then be created using an axiom like
the excision axiom in Zermelo-Fraenkel set theory.
\item Type theory is \emph{typed} while set theory is \emph{untyped}.
While in set theory, objects can be an element of different sets --
consider the number two which is an element both of the set of even integers
as well as the set of all integers --
type theory is based on the principle that every piece of data (every term)
is assigned a unique type which is known at the point of the creation of the data.
Since this assignment, called typing, is decidable, we consider it a judgment
rather than a provable proposition that a given term $t$ has type $A$.
\item Type theory is inherently \emph{constructive} while most of the most
well-known set theoretic foundations are \emph{non-construcitve}.
This has great consequences for the computational use of represented mathematics:
Every type theoretic function the domain of which are the natural numbers, can compute a
numeral for any given input.
\item While in set theory, \emph{sets ar the only primitives}, type theory
provides often-used objects like \emph{functions and inductively defined types
as primitives}.
This means that we can use these without having to care about how to assemble them
from other primitives. 
It also offers modularity in the follwowing sense: %TODO find a better word than modularity
In constrast to a function in set theory, a function in type theory only reveals
its constituing information:
What is the resulting output for a given input?
\end{itemize} %TODO maybe say something about sizes

Let us now fix some notation and some basic constructions which will occur
throughout this text.
As mentioned, we will, whenever we talk about a certain piece of type theoretic
data, accompany it with its type,
we need a notation for this kind of \textbf{type judgment}:
We write $t : A$ to state that the term $t$ is of type $A$.
Sometimes, we want to express that two terms $s$ and $t$ of the same type $A$
only differ an unfolding or folding of a definition, or that the application of
a reduction rule to $s$ results in $t$.
We will notate this by $s \equiv t$.
Our type theory will then make no distinction between $s$ and $t$.
This means that if $A$ and $B$ are types with $A \equiv B$, then
$s : A$ implies $s : B$.
To keep type checking decidable it is easy to see that it is important
to keep the question whether two terms are what we call \textbf{definitionally equal}
decidable itself.

We said above that each piece of data, so each term,
has a unique type (up to definitional equality).
This statement also holds true for types itself.
Types which themselves contain types, are called \textbf{universes} and we will
denote them using $\UU$.
The universe itself also needs a type, but
assuming $\UU : \UU$ is inconsistent (this is often referred to as
``Girard's paradox'' (\cite{girard72,hurkens95}) which can be seen as the type theoretic equivalent
to Russell's paradox).
So the solution we apply for the remainder of this text is
to assume that we have an infinte chain of universes
\begin{equation*}
\UU_0 : \UU_1,~~ \UU_1 : \UU_2,~~ \UU_2 : \UU_3,~~ \ldots \text{,}
\end{equation*}
each contained in the next one.
Most often, we will chose to leave the index implicit and regard our constructions
as being \emph{universe polymorphic}, meaning that they are valid in any chosen
universe.
Some type theories are constructed to be \emph{cumulative} in the sense that whenever
we have a type $A : \UU_i$, it is a type in the succeeding universe, so $A : \UU_{i+1}$.
Our constructions will not rely on cumulativity and they are formalized in
non-cumulative type theories.

Until now, we have not talked about any way to form types.
In the following we will get to know the non-dependent version of some basic types,
some of which will later be generalized to a dependent form.
These type formers will be presented in a very similar way:
We will give a rule on how to form the type itself (called formation rule),
rules on how to construct elements of the type (called introduction rules),
rules on how to use the elements of the type (called elimination rules),
and some of them will be followed by some reduction rules offering definition
equalities used to simplify terms.
As we have mentioned in the comparison to set theory, functions are basic building
blocks in type theory.
The non-dependent functions form what is known as \emph{simply typed $\lambda$-calculus}.
The formation, introduction, and elimination rules correspond to the fact that
we can form the function type of any type for its domain and any type for its
codomain, the fact that we can build a function by
specifying its output for any given input, and the fact that we can apply
a function to any term of its domain:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\to$-Form]{A,B : \UU}{A \to B : \UU} \qquad
\inferrule*[left=$\to$-Intro]{a : A \vdash \Phi[a/x] : B}{(\lambda x. \Phi) : A \to B} \\[.7em]
\inferrule*[left=$\to$-Elim]{f : A \to B \\ a : A}{f(a) : B}
\end{gathered}
\end{equation*}
Here, $\Phi$ is a term that may have $x$ as a free variable.
$\Phi[a/x]$ denotes the replacement by every appearance of $x$ by $a$.
Additional to these rules we also have $\eta$-conversion and $\beta$-reduction:
\begin{equation*} \label{eq:eta-beta}
\begin{aligned}
\inferrule*[left=$\eta$]{f : A \to B}{(\lambda x. f(x)) \equiv f} \qquad
\inferrule*[left=$\beta$]{(\lambda x. \Phi) : A \to B \\ a : A}
	{(\lambda x. \Phi)(a) \equiv \Phi[a/x]}
\end{aligned}
\end{equation*}
In type theory, these functions are used to represent
both functions between sets as well as implications between propositions.

\begin{remark}\label{rmk:tt-syntax}
When giving inference rules as the ones above, we always assume an arbitray
\emph{context} of variables for their premises as well as their conclusion.
Often, when presenting the syntax of a type theory, rules on how
to form these contexts $\vdash \Gamma$ are given together with rules for the
formation of types $\Gamma \vdash A$ in a given context $\Gamma$ and the
treatment of terms $\Gamma \vdash t : A$ of a given type $A$ in context $\Gamma$.
We will choose to be immplicit about the variable context in this chapter, while
being more explicit about them when we introduce other syntaxes in later chapter,
for which a more formal treatment is appropriate.
\end{remark}

\section{Homotopy Type Theory}\label{sec:tt-hott}

\section{Theorem Provers Based on Type Theory}\label{sec:tt-provers}








