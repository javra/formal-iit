This chapter shall serve to introduce the basic notions of type theory which
we will need for the subsequent content of this thesis.
At first (Chapter~\ref{sec:tt-dtt}), we will have a general look on dependent type theory, its use and
how it differs from a set theoretic foundation,
afterwards (Chapter~\ref{sec:tt-w}) we want to unify the notion of an inductive
type using the concept of indexed W-types.
Then (Chapter~\ref{sec:tt-hott}), we will explain how homotopy type theory was created to have a suitable
language to reason about higher equalities and how it provides a synthetic way to
formalize topological insights.
Finally, in Chapter~\ref{sec:tt-provers}, we will give examples of two theorem
provers based on dependent types, Agda and Lean, and point out some of their differences.

\section{Dependent Type Theory}\label{sec:tt-dtt}

The term ``type theory'' stems from the early nineteenth centry, when
Bert\-rand Russell sought to lay out an alternative form of set theory which did not
suffer from the paradox which Russell discovered.
Todays versions of type theory have little in common with Russell's attempts
but rather rely on the considerations of Per Martin-Löf (\cite{martin-lof2, martin-lof1})
who, starting in the 1970's, built a new mathematical foundation based on the
$\lambda$-calculus, himself drawing inspiration from previous logicians and
mathematicians like Alonzo Church and Haskell Curry.
Often, dependent type theory is also referred to as Martin-Löf type theory (MLTT).

Based on a phenomenon known as the ``Curry-Howard corres\-pon\-dence'', type theory
can serve both as a theoretical foundation for a formal representation of mathematics,
as well as a principle for the specification for strongly typed functional
programming languages.
It was implemented in computer languages for programming and theorem proving which
are massively used in the field of formal verification and in the formalization of
mathematics.
Among the most commonly used implementations are the theorem prover Coq (\cite{coq})
which notably has a lot of users in the field of hardware verification,
the prover Agda (\cite{agda}), which is popular amongst type theorists themselves,
and the Microsoft Research based project Lean(\cite{mouracade}), which has
drawn considerable attention from researching mathematicians as a tool to
formally verify their proofs.

Type theory differs from a set theoretic mathematical foundation (let us, as a
point of reference, consider set theories based on first-order predicate logic
like Zermelo-Fraenkel set theory) in several
important aspects:
\begin{itemize}
\item Type theory follows a paradigm called \emph{``propositions-as-types''}.
This means that statements like theorems and conjectures are represented
using the same class of objects as other data like sets or (algebraic) structures.
In contrast to this, most set theoretic foundations are built on a \emph{dichotomy between
the propositions and the objects} they describe:
They first start out with a logical framework on which axiomatically a theory
of sets is introduced.
The coherence between these two levels must then be created using an axiom like
the comprehension axiom in Zermelo-Fraenkel set theory.
\item Type theory is \emph{typed} while set theory is \emph{untyped}.
While in set theory, objects can be an element of different sets --
consider the number two which is an element both of the set of even integers
as well as the set of all integers --
type theory is based on the principle that every piece of data (every term)
is assigned a unique type which is known at the point of the creation of the data.
This assignment, called typing, is decidable, and we consider it a judgment
rather than a provable proposition that a given term $t$ has type $A$.
\item Type theory is inherently \emph{constructive} while many
set theoretic foundations, such as Zermelo-Fraenkel, are \emph{non-constructive}.
This has great consequences for the computational use of the mathematics represented:
Every type theoretic function the codomain of which are the natural numbers, can compute a
numeral for any given input.
\item While in set theory, \emph{sets are the only primitives} and all data is
encoded as sets, type theory
provides often-used objects like \emph{functions and inductively defined types
as primitives}.
This means that we can use these without having to care about how to assemble them
from other primitives. 
%It also offers modularity in the following sense: %TODO find a better word than modularity
%In contrast to a function in set theory, a function in type theory only reveals
%its constituing information:
%What is the resulting output for a given input?
\end{itemize} %TODO maybe say something about sizes

Let us now fix some notation and some basic constructions which will occur
throughout this text.
As mentioned, we will, whenever we talk about a certain piece of type theoretic
data, accompany it with its type,
we need a notation for this kind of \textbf{type judgment}:
We write $t : A$ to state that the term $t$ is of type $A$.
This is similar to the element relation of a set, but it also represents the fact
that $t$ is a proof for a proposition $A$.
Sometimes, we want to express that two terms $s$ and $t$ of the same type $A$
only differ by an unfolding or folding of a definition, or that the application of
a reduction rule to $s$ results in $t$.
We will notate this by $s \equiv t$.
Our type theory will then make no distinction between $s$ and $t$.
This means that if $A$ and $B$ are types with $A \equiv B$, then
$s : A$ implies $s : B$.
To keep type checking decidable it is easy to see that it is important
to keep the question whether two terms are what we call \textbf{definitionally equal}
decidable itself.

We said above that each piece of data, so each term,
has a unique type (up to definitional equality).
This statement also holds true for types itself.
Types which themselves contain types, are called \textbf{universes} and we will
denote them using $\UU$.
The universe itself also needs a type, but
assuming $\UU : \UU$ is inconsistent.
This is often referred to as
``Girard's paradox'' \citep{girard72,hurkens95} which can be seen as the type theoretic equivalent
to Russell's paradox.
The solution we assume for the remainder of this text is
to assume that we have an infinite chain of universes
\begin{equation*}
\UU_0 : \UU_1,~~ \UU_1 : \UU_2,~~ \UU_2 : \UU_3,~~ \ldots \text{,}
\end{equation*}
each contained in the next one.
Most often, we will chose to leave the index implicit and regard our constructions
as being \emph{universe polymorphic}, meaning that they are valid in any chosen
universe.
Some type theories are constructed to be \emph{cumulative} in the sense that whenever
we have a type $A : \UU_i$, it is a type in the succeeding universe, so $A : \UU_{i+1}$.
Our constructions will not rely on cumulativity and they are formalized in
non-cumulative type theories.

Until now, we have not talked about any way to form types.
In the following, we will get to know the non-dependent version of some basic types,
some of which will later be generalized to a dependent form.
These type formers will be presented in a very similar way:
We will give a rule on how to form the type itself (called formation rule),
rules on how to construct elements of the type (called introduction rules),
rules on how to use the elements of the type (called elimination rules),
and some of them will be followed by some reduction rules offering definitional
equalities used to simplify terms.
As we have mentioned in the comparison to set theory, functions are basic building
blocks in type theory.
The non-dependent functions form what is known as \emph{simply typed $\lambda$-calculus}.
The formation, introduction, and elimination rules correspond to the fact that
we can form the function type of any type for its domain and any type for its
codomain, the fact that we can build a function by
specifying its output for any given input, and the fact that we can apply
a function to any term of its domain:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\to$-Form]{A,B : \UU}{A \to B : \UU} \qquad
\inferrule*[left=$\to$-Intro]{a : A \vdash \Phi[a/x] : B}{(\lambda x. \Phi) : A \to B} \\[.7em]
\inferrule*[left=$\to$-Elim]{f : A \to B \\ a : A}{f(a) : B}
\end{gathered}
\end{equation*}
Here, $\Phi$ is a term that may have $x$ as a free variable.
$\Phi[a/x]$ denotes the replacement of every free appearance of $x$ by $a$.
Additional to these rules we also have $\eta$-conversion and $\beta$-reduction:
\begin{equation*} \label{eq:eta-beta}
\begin{aligned}
\inferrule*[left=$\eta$]{f : A \to B}{(\lambda x. f(x)) \equiv f} \qquad
\inferrule*[left=$\beta$]{(\lambda x. \Phi) : A \to B \\ a : A}
	{(\lambda x. \Phi)(a) \equiv \Phi[a/x]}
\end{aligned}
\end{equation*}
In type theory, these functions are used to represent
both functions between sets as well as implications between propositions.
A special kind of function are those of the form $B : A \to \UU$.
These so called \textbf{type families} are used to represent set-valued
functions as well as propositions with a free variable in $A$.

\begin{remark}\label{rmk:tt-syntax}
When giving inference rules as the ones above, we always assume an arbitrary
\emph{context} of variables for their premises as well as their conclusion.
Often, when presenting the syntax of a type theory, rules on how
to form these contexts $\vdash \Gamma$ are given together with rules for the
formation of types $\Gamma \vdash A$ in a given context $\Gamma$ and the
treatment of terms $\Gamma \vdash t : A$ of a given type $A$ in context $\Gamma$.
We will choose to be implicit about the variable context in this chapter, while
being more explicit about them when we introduce other syntaxes in later chapter,
for which a more formal treatment is appropriate.
\end{remark}

Since we want a representation of logics in type theory, and so far the only logical
connective we introduced are implications, we might next ask for types representing
the propositions ``true'' and ``false''.
Since the elements of this type should correspond to the proofs of ``true'' and
``false'', we can conclude that the type corresponding to ``true'' should contain
one canonical element, while the one corresponding to ``false'' should contain
none.
Thus, we will call these two types the \textbf{empty type} and the \textbf{unit type},
respectively and denote them with $\emptytype$ and $\unit$.
The formation and introduction rules for these are not very surprising:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\emptytype$-Form]{ }{\emptytype : \UU} \qquad
\inferrule*[left=$\unit$-Form]{ }{\unit : \UU} \qquad
\inferrule*[left=$\unit$-Intro]{ }{\star : \unit} \\[.7em]
\end{gathered}
\end{equation*}
The elimination rule for $\emptytype$ says the we can derive a proof for any statement,
given a proof of ``false'' (``ex falso quodlibet''), while the elimination rule
for $\unit$ specifies that when showing a statement about the elements of the
unit type, it suffices to consider its canonical element $\star$:
\begin{equation*}
\inferrule*[left=$\emptytype$-Elim]{C : \emptytype \to \UU \\ x : \emptytype}
	{\elim_\emptytype(C,x) : C(x)} \qquad
\inferrule*[left=$\unit$-Elim]{C : \unit \to \UU \\ p : C(\star) \\ x : \unit}
	{\elim_\unit(C,p,x) : C(x)}
\end{equation*}
with the reduction rule $\elim_\unit(C,p,\star) \equiv p$.
Some type theories assume so called $\eta$-rules for types like $\unit$, which
say that for every $x : \unit$, we have $x \equiv \star$.
We will \emph{not} assume these rules to hold, but instead we will later be able to
express the fact that all instances of $\unit$ are equal to $\star$ using propositional
equality.

To give an example for a type which contains more than just one element, we can
consider the type of booleans $\twotype$, containing elements $0_\twotype$
and $1_\twotype$.
This time, we have more than just one constructor, and the elimination rule
requires us to give proofs for both of the two elements:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\twotype$-Form]{ }{\twotype : \UU} \qquad
\inferrule*[left=$\twotype$-Intro1]{ }{0_\twotype : \twotype} \qquad
\inferrule*[left=$\twotype$-Intro2]{ }{1_\twotype : \twotype} \\[.7em]
\inferrule*[left=$\twotype$-Elim]{C : \twotype \to \UU \\ p_0 : C(0_\twotype) \\ p_1 : C(1_\twotype) \\ x : \twotype}
  {\elim_\twotype(C, p_0, p_1, x) : C(x) }
\end{gathered}
\end{equation*}
with the reduction rules $\elim_\twotype(C, p_0, p_1, 0_\twotype) \equiv p_0$
and $\elim_\twotype(C, p_0, p_1, 1_\twotype) \equiv p_1$.

As a minimal example of an infinite type, the set theoretic equivalent of which
would be introduced axiomatically, we can take a look at the type of
natural numbers.
It is generated inductively by the zero element and the successor function.
The eliminator provides the principle of induction on the natural numbers, which
also makes sure that, when proving properties about the natural numbers,
we can assume that every element of the natural numbers is equal to a repeated
applying the successor function to zero:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\N$-Form]{ }{\N : \UU} \qquad
\inferrule*[left=$\N$-Intro]{ }{0 : \UU \\ S : \N \to \N} \\[.7em]
\inferrule*[left=$\N$-Elim]
	{C : \N \to \UU \\ p : C(0) \\ q_n : C(n) \to C(S(n)) \text{ for all $n : \N$} \\
		x : \N}
	{\elim_\N(C, p, q, x) : C(x)} %TODO find a better solution to replace the pi type
\end{gathered}
\end{equation*}
with the reduction rules
\begin{align*}
\elim_\N(C, p, q, 0) &\equiv p \text{ and} \\
\elim_\N(C, p, q, S(x)) &\equiv q(x, \ind_\N(C, p, q, x)) \text{.}
\end{align*}
Note that we regain, by restricting $\elim_N$ to constant type families,
the \emph{non-dependent eliminator} or \emph{recursor},
corresponding to the usual way to recursively define functions $f : \N \to A$ for
a type $A : \UU$ by providing the value $f(0)$ and, for each $n : \N$ the
recursive definition $f(n) \to f(S(n))$.
It has the following type:
\begin{equation*}
\rec_\N(A) \equiv \ind(\lambda x. A) : A \to (\N \to A \to A) \to \N \to A \text{.}
\end{equation*}

So far we did not do the word ``dependent'' in the name of the type theory
much justice,
but now we will move on to introduce some dependent type formers.
The first of these will be \textbf{$\Pi$-types} or \text{dependent function types}.
When considering non-dependent functions, the codomain was a fixed type $B$
such that for all inputs $a : A$, the output $f(a)$ is an element of $B$.
For dependent functions, however, the codomain is a type family $B : A \to \UU$,
and for each input $a : A$ the output $f(a)$ is of type $B(a)$.
Considering that we want to use type families to represent propositions depending on
a free variable, these functions represent the universal quantification over this
free variable!
While for application and $\lambda$-abstraction we don't introduce new notation
for dependent functions, we will denote the type of all dependent functions
on a type family $B : A \to \UU$ as $\prod_{(a : A)} B(a)$, or, alternatively,
as $\Pi(B)$ (as a shorter variant) or $(a : A) \to B(a)$ (often called
``Agda-notation'' in referral to the theorem prover of that name).
The rules to form the type of $\Pi$-types and to introduce and apply dependent
functions generalize the rules for non-dependent functions as follows:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\Pi$-Form]{A : \UU_i \\ B : A \to \UU_j}
	{\textstyle{(\prod_{(a : A)} B(a))} : \UU_{\max\{i,j\}}} \qquad
\inferrule*[left=$\Pi$-Intro]{a : A \vdash \Phi[a/x] : B(a)}
	{(\lambda x. \Phi) : \textstyle{\prod_{(a : A)} B(a)}} \\[.7em]
\inferrule*[left=$\Pi$-Elim]{f : \textstyle{\prod_{(a : A)} B(a)} \\ a : A}
	{f(a) : B(a)}
\end{gathered}
\end{equation*}
Again, we have the rules for $\beta$-reduction and $\eta$-conversion like in
the non-dependent case, yielding reduction rules in the form of judgmental equalities
$(\lambda x. f (x)) \equiv f$ and $(\lambda x. \Phi)(a) \equiv \Phi[a/x]$.
Having $\Pi$-types at our disposal allows us to state the rules governing
a couple of further essential type formers.
Note that, once we have added dependent functions, we can rediscover
non-dependent functions as the special case of dependent functions over
a constant type family.
When iterating $\Pi$-types,
we will often find that the argument of a dependent function is already
determined by an earlier argument, as in
$f : (a : A)(b : B(a)) \to C(a, b)$.
In this case, we borrow the notation used by many theorem provers and
use curly brackets to denote arguments which will be left implicit:
If $f :\{a : A\}(b : B(a)) \to C(a, b)$ and $b : B(a)$,
we write $f(b) : C(a, b)$.
If we later want to state these explicitly we will re-use curly brackets
to denote that we reintroduce them: $f\{a\}(b) : C(a, b)$.

Two important logical connectives are still missing:
Conjunction and disjunction. In type theory these coincide with
the product and disjoint union (sum) of types.
The rules for the product type are as follows:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\times$-Form]{A, B : \UU}{A \times B : \UU} \qquad
\inferrule*[left=$\times$-Intro]{a : A \\ b : B}{(a, b) : A \times B} \\[.7em]
\inferrule*[left=$\times$-Elim]{C : A \times B \to \UU \\
	p: (a : A)(b : B) \to C(a,b) \\ x : A \times B}
	{\elim_{A \times B}(C, p, x) : C(x)}
\end{gathered}
\end{equation*}
with the reduction rule $\elim_{A \times B}(C, p, (a, b)) \equiv p(a, b)$.
The projections of an instance $x : A \times B$ are then
defined by induction:
\begin{align*}
\pr_1(x) &:\equiv \elim_{A \times B}((\lambda y. A), (\lambda a. \lambda b. a), x) : A \text{ and}\\
\pr_2(x) &:\equiv \elim_{A \times B}((\lambda y. A), (\lambda a. \lambda b. b), x) : B \text{,}
\end{align*}
yielding $\pr_1((a, b)) \equiv a$ and $\pr_2((a, b)) \equiv b$ judgmentally.
The type representing disjunction has two constructors determining whether
we provide proof for its left or its right type:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$+$-Form]{A : \UU \\ B : \UU}{A + B : \UU} \\[.7em]
\inferrule*[left=$+$-Intro1]{a : A}{\inl(a) : A + B} \qquad
\inferrule*[left=$+$-Intro2]{b : B}{\inr(b) : A + B} \\[.7em]
\inferrule*[left=$+$-Elim]
	{C : (A + B) \to \UU \\  p : (a : A) \to C(\inl(a))
		\\ q : (b : B) \to C(\inr(b)) \\ x : A + B}
	{\elim_{A + B}(C, p, q, x) : C(x)}
\end{gathered}
\end{equation*}
For the sum type, we have the reduction rules
\begin{align*}
\elim_{A + B}(C, p, q, \inl(a)) &\equiv p(a) \text{ and} \\
\elim_{A + B}(C, p, q, \inr(b)) &\equiv q(b) \text{.}
\end{align*}

Looking at the product type, we can find a generalization which is very useful
when we want to model existential quantification of a type family $B : A \to \UU$
in type theory:
A version of the product type where the type of the second component of a pair $(a, b)$
may depend on $a : A$ via the type family, i.\,e. $b : B(a)$.
The type holding this kind of pair for a fixed type family $B : A \to \UU$
is called the \textbf{$\Sigma$-type} over $B$.
We will again have three different notations for this type, of which in this text
we will mostly prefer the latter one:
The type is usually \citepalias{hottbook} denoted by
$\sum_{(a : A)} B(a)$, mimicing mathematical notation for sums.
Furthermore there is the short variant of writing $\Sigma(B)$ and
an Agda-inspired notation \mbox{$(a : A) \times B(a)$}.
The inference rules for this type are just a slight generalization of the
rules we have already seen for the non-dependent product type:
\begin{equation*}
\begin{gathered}
\inferrule*[left=$\Sigma$-Form]{A : \UU \\ B : A \to \UU}
	{(a : A) \times  B(a) : \UU} \qquad
\inferrule*[left=$\Sigma$-Intro]{a : A \\ b : B(a)}
	{(a, b) : (a : A) \times  B(a)} \\[.7em]
\inferrule*[left=$\Sigma$-Elim]
	{C : (a : A) \times  B(a) \to \UU \\
		p : (a : A)(b : B(a)) \to C((a,b)) \\
		x :  (a : A) \times B(a) }
	{\elim_{(a : A) \times B(a)}(C, p, x) : C(x)}
\end{gathered}
\end{equation*}
As our notation already suggests, we can view the product $A \times B$ as a special
case of a $\Sigma$-type where $B$ is a constant type family.
Note that also the sum type $A + B$ can be defined as a special case of the sigma
type over $C : \twotype \to \UU$ with $C(0_\twotype) :\equiv A$
and $C(1_\twotype) :\equiv B$.
This is why $\Sigma$-types are sometimes also referred to as
\emph{dependent sum types}.
By induction we can define the \textbf{projections}
\begin{align*}
\pr_1 &: ((a : A) \times B(a)) \to A \text{ and} \\
\pr_2 &: (x : (a : A) \times B(a)) \to B(\pr_1(x)) \text{.}
\end{align*}
which return the components of the dependent pairs.
When we iterate $\Sigma$-types an products we will be liberal with the notation
and allow notation like $\pr_3, \ldots$ as well.

With the type formers we met so far, we can already represent a lot of mathematical
definitions and knowledge.
For example, if we wanted to define what it means for a natural number to be odd,
we could set
\begin{equation*}
\isodd :\equiv \rec_\N(\UU, \emptytype, (\lambda n. \lambda A. A \to \emptytype)) : \N \to \UU \text{.}
\end{equation*}
For example, this gives us the statement that the number one is odd, witnessed by
the following term:
\begin{align*}
(\lambda x. x) :~ & \emptytype \to \emptytype \\
 &\equiv \elim_\N((\lambda x. \UU), \emptytype, (\lambda n. \lambda A. A \to \emptytype), 0)
  \to \emptytype\\
 &\equiv \elim_\N((\lambda x. \UU), \emptytype, (\lambda n. \lambda A. A \to \emptytype), S(0))\\
 &\equiv \isodd(S(0)) \text{.}
\end{align*}

\section{Inductive Types}\label{sec:tt-w}

So far, the presented type formers may seem like a zoo of unrelated,
random examples.
Some are generalization of others (like $\Sigma$-types generalize sums),
but the question one might ask is if there is any overarching principle behind
the choice of those type formers.
The feature that all of them have in common is that, rather than by an enumeration
of their elements,
they are defined by their formation, introduction, and elimination rules -- their
elements are those which are generated \emph{inductively} by their introduction
rules, also called \emph{constructors}.

Some of the type formers we have seen were \emph{parameterized} by other types,
but more than that, dependent types allow us to specify \emph{indexed} inductive
types.
One example for this is the type of \emph{vectors} of a type $A$.
Vectors are a variant of lists, where we use the typing to keep track of the
length of its elements:
The type of vectors on $A$ is not a type but a type family:
\begin{equation*}
\vecty_{A} : \N \to \UU \text{.}
\end{equation*}
It has two constructors and an eliminator of the following form:
\begin{equation*}
\begin{gathered}
\inferrule{ }
  {\nil : \vecty(0)}
\qquad
\inferrule{n : \N \\ a : A \\ v : \vecty(n)}
  {\cons(a, v) : \vecty(n + 1)} \\[.7em]
\inferrule{C : \{n : \N\} \to \vecty(n) \to \UU \\
  p_\nil : C(\nil) \\
  p_\cons : \{n : \N\}(a : A)(v : \vecty(n)) \to C(v) \to C(\cons(a,v)) \\
  n : \N \\ v : \vecty(n)}
  {\elim_\vecty(C, p_\nil, p_\cons, v) : C(v)}
\end{gathered}
\end{equation*}
with two reduction rules
\begin{align*}
\elim_\vecty(C, p_\nil, p_\cons, \nil) &\equiv p_\nil \text{ and} \\
\elim_\vecty(C, p_\nil, p_\cons, \cons(a, v)) &\equiv p_\cons(a, v, \elim_\vecty(C, p_\nil, p_\cons, v)) \text{.}
\end{align*}
In natural language, the eliminator says that to show a statement about vectors
it is sufficient to prove it about the empty vector and that the statement is stable
under extending an arbitrary vector.
Note that the natural number in the conclusion of both introduction rules
is not a variable, and the $\cons$ even modifies the natural number.
As such, we call the dependency on the length in the type of vectors
an \emph{index} instead of a parameter.

While dependent and non-dependent functions were primitives needed to even talk
about other type formers,
all other examples which we have seen, can be captured by the concept of
\emph{indexed inductive type}.
But since we want to be more formal than to just assume the presence of indexed
inductive types based on giving a few examples,
we will present different attempts to make precise a definition of what
an indexed inductive type is.
One schematic approach was made by \cite{dybjer94}, while the approach which
we want to introduce here requires an encoding of the constructors in a very specific
way to be able to present the inductive type as an instance of a very general
form of a \emph{tree}.
The types covered by this approach are called \emph{indexed W-Types}, and they
cover a bigger fragment of inductive types than the non-indexed version,
which are also known as ``Petersson-Synek trees'' \citep{petersson1989set},
and which are in a topos theoretic setting, discussed by \cite{moerdijkwellfounded}.
The generalization to indexed W-Types, as presented here,
was found by \cite{indexedcontainers}.

\begin{defn}
Assume that we are given the following data:
\begin{itemize}
\item A type $I : \UU$ of \emph{indices},
\item a type $A : \UU$ encoding the number and non-recursive input \emph{data} or \emph{shapes} for contructors,
\item a function $o : A \to I$ assigning to each piece of input data the corresponding \emph{output index} of
the constructor,
\item a type $B : A \to \UU$ of \emph{recursive occurrences} or \emph{positions} for each
bit of input data, and
\item a function $r : (a : A) \to B(a) \to I$ of indices of \emph{recursive occurrences}.
\end{itemize}
Then, we assume that we have
a type $\IW{A}{B}{o}{r} : I \to \UU$ with the following introduction and elimination
rules:
\begin{equation*}
\begin{gathered}
\inferrule*[left=IW-Intro]{a : A \\
  c : (b : B(a)) \to \IW{A}{B}{o}{r}(r(a, b))}
  {\IWsup{a}{c} : \IW{A}{B}{o}{r}(o(a))} \\[.7em]
\inferrule*[left=IW-Elim]{C : \{i : I\} \to \IW{A}{B}{o}{r}(i) \to \UU \\
  \makecell{p :  (a : A)
      \left (c : (b : B(a)) \to \IW{A}{B}{o}{r}(r(a, b))\right) \\
      \qquad \qquad \to \Bigl((b : B(a)) \to  C(c(b))\Bigr)
      \to C(\IWsup{a}{c})  } }
  {\elim_\mathsf{IW}(C, p) : (i : I)(w : \IW{A}{B}{o}{r}(i)) \to C(w) }
\end{gathered}
\end{equation*}
We furthermore assume that we are provided the reduction rule
\begin{equation*}
\elim_\mathsf{IW}(C, p, o(a), \IWsup{a}{c})
  \equiv p(a, c, \lambda b : B(a).\, \elim_\mathsf{IW}(C, p, r(a,b), c(b))) \text{.}
\end{equation*}
\end{defn}

This definition may seem very confusing and overly complicated, but this is necessary
to capture all possible indexed inductive types in full generality.
In words, the constructor describes that we chose a constructor and giving possible
non-recursive input data by providing $a : A$,
and then, based on this data, give data for all recursive occurrences of the type
in a constructor in the form of $c$, we get a new element $\IWsup{a}{c}$
which is reminiscent of the \emph{supremum} of the given data.
Conversely the elimination rule describes that to proof a statement $C$ about this
tree like structure it is enough to prove $C$ for $\IWsup{a}{c}$
under the hypothesis that it holds for all recursive input data $c(b)$.

Assuming that we already have $\emptytype$, $\unit$, $\twotype$ and $\Sigma$-types
at our disposal (allowing us to define $A + B$),
indexed W-types live up to our expectations of capturing
all previously mentioned inductive types.
To provide a translation of these, including indexed inductive types still to
be defined in the next chapter, we will provide all the
arguments for the respective indexed W-types in Table~\ref{tbl:tt-iws},
while often, in the case of $I \equiv \unit$ describe
$\unit \to A$ in place of a type $A$ itself.

\begin{remark}[Plain W-Types]
Indexed W-types can be reduced to the simpler class of \emph{W-types},
which only are specified by giving a type $A : \UU$ and a family $B : A \to \UU$,
without having a type of indices $I : \UU$, together with $o$ and $r$.
We can think of them as the class of indexed W-types where $I \equiv \unit$.

The fact that we can present each indexed W-types by a non-indexed one was shown
by \citet{indexedcontainers} using the K-rule,
and by \citet{Sattler:indexedW} without presence of the K-rule and under assumption
of functional extensionality.
\end{remark}

In Chapters~\ref{chp:iit} and \ref{chp:if} we will introduce two more calculi to
replace indexed W-types.

\begin{sidewaystable}
\centering
{$\begin{array}{r|lllll}
\text{Type Former} & I : \UU & A : \UU & B : A \to \UU & o : A \to I & r : (a : A) \to B(a) \to I \\
\hline
\N
  & \unit
  & \makecell[cl]{\unit \\ + \unit}
  & \makecell[cl]{\inl(\star) \mapsto \emptytype \\ \inr(\star) \mapsto \unit}
  & \_ \mapsto \star
  & \_ \mapsto \star \\
\vecty_A
  & \N
  & \makecell[cl]{\unit \\ + A \times \N}
  & \makecell[cl]{\inl(\star) \mapsto \emptytype \\ \inr(a, n) \mapsto \unit}
  & \makecell[cl]{\inl(\star) \mapsto 0          \\ \inr(a, n) \mapsto n + 1}
  & \makecell[cl]{ - \\ \inr(a, n) \mapsto (\star) \mapsto n} \\
x =_A \_
  & A
  & \unit
  & \star \mapsto \emptytype
  & \star \mapsto x
  & -
\end{array}$}
\caption{The input data for the indexed W-types corresponding to the type formers
given in Chapter~\ref{sec:tt-dtt}.}\label{tbl:tt-iws}
\end{sidewaystable}

\section{Typal Equality and Homotopy Type Theory}\label{sec:tt-hott}

Until now, the only equations we encountered were judgmental equalities which
aren't ``visible'' internally in the type theory.
But since we want to follow the paradigm of propositions-as-types, we also want to
have a way to represent the statement that two terms of a type $A : \UU$ are
equal \emph{inside} the type theory.
To correct this,
we introduce what is usually called \textbf{propositional equality} or
\textbf{typal equality}.
For each type $A : \UU$ and each two elements $a, b : A$ we want to have
a type $(a =_A b) : \UU$ of proofs that $a$ and $b$ are equal.
We can view this as an inductive definition, giving only the witness for this relation
to be reflexive as a constructor:
\begin{equation*}
\begin{gathered}
\inferrule*[left={$=$-Form}]{A : \UU \\ a, b : A}{a =_A b : \UU} \qquad
\inferrule*[left={$=$-Intro}]{A : \UU \\ a : A}{\refl_a : a =_A a} \\[.7em]
\inferrule*[left={$=$-Elim}]
	{a : A \\ C : (b : A) \to a =_A b \to \UU \\
		c : C(a, \refl_a) \\
		b : A \\ p : a =_A b}
	{\elim_{=_A}(a, C, c, b, p) : C(b, p)}
\end{gathered}
\end{equation*}
with the reduction rule $\elim_{=_A}(C, c, a, p, \refl_a) \equiv c(a)$.
It can be defined as an indexed W-type as per \Cref{tbl:tt-iws}.
The elimination rule says that to prove a statement indexed over varying right
hand sides of an equality and corresponding equality proofs,
we can assume it to be the reflexivity witness $\refl$.

\begin{remark}\label{rmk:tt-k}
Apart from the above elimination rule, which is sometimes referred to as
\textbf{J-rule}, some type theories also assume the so called
\textbf{K-rule}, which in proofs also allow us to simplify a given equality proof
to $\refl$ if the type family which we want to inhabit varies both endpoints
of the equality simultaneously, as in the following:
\begin{equation*}
\inferrule*[left={$=$-K}]{C : (a : A) \to a =_A a \to \UU \\
    c : (a : A) \to C(a, \refl_a) \\
    a : A \\ p : a =_A a}
  {\elim'_{=_A}(C, c, a) : C(a) }
\end{equation*}
We will assume this rule to hold in Chapter~\ref{chp:iit} and later chapters,
but explicitly assume its absence in this and the following two chapters.

In literature, our version of \textsc{$=$-Elim} is often called the
\emph{based} J-rule, because the equation's left hand side is fixed,
or Paulin-Mohring J-rule, after \cite{Moh93}.
An alternative, but provably equivalent version is the so-called
\textbf{unbased J-rule} in which the type family varies over both sides
of the equation:
\begin{equation*}
\inferrule*[left={$=$-Elim'}]
	{C : (a, b : A) \to a =_A b \to \UU \\
		c : C(a, a, \refl_a) \\
		a, b : A \\ p : a =_A b}
	{\elim'_{=_A}(C, c, a, b, p) : C(b, p)}
\end{equation*}
\end{remark}

While we introduced types as a means to represent sets and propositions,
passing on the K-rule also gives rise to a further interpretation:
We can use types to model the homotopy types of topological spaces.
This principle, together with the univalence axiom which we will introduce
in the next chapter, is the underlying insight of the field of \emph{homotopy type theory}.
This correspondence was first explored by \cite{awodey2009homotopy}
and made precise by \cite{kapulkinlumsdaine},
who, after an idea by Vladimir Voevodsky,
modelled homotopy type theory using simplicial sets.
In this correspondence -- a succinct list of which can be found in Table~\ref{tbl:tt-hott} --
equality proofs correspond to \textbf{paths}, a name which we will use as a synonym
from using from now on.

\begin{sidewaystable}
{\small
\begin{tabular}{r|ccc}
  & Logical Interpretation & Set Interpretation & Homotopical Interpretation \\
\hline
(closed) Type $A : \UU$
  & Proposition
  & Set
  & Topological Space \\
Function $f : A \to B$
  & Proof of implication
  & Function between sets
  & Continuous map \\
Type family $B : A \to \UU$
  & Proposition with free variable in $A$
  & Family of sets indexed by $A$
  & Fibration over base $A$ \\
Pair $(a, b) : A \times B$
  & Proof of conjunction
  & Pair in cartesian product
  & Point in product space \\
Element $\inl(a) : A + B$
  & Proof of disjunction
  & Element of disjoint union
  & Point in disjoint union \\
Dep. fn. $f : \Pi(B)$
  & Proof of universal quantification
  & Dep. set valued function
  & Section of fibration \\
Dep. pair $(a, b) : \Sigma(B)$
  & Proof of existential quantification
  & Dependent product
  & Point in total space of fibration \\
Equality $p : a =_A b$
  & --
  & Equality in $A$
  & Path from $x$ to $y$ \\
Equivalence $f : A \simeq B$
  & Proof of biconditional
  & Bijection
  & Homotopy equivalence
\end{tabular}}
\caption{Three interpretations}\label{tbl:tt-hott}
\end{sidewaystable}

But what does equality, as defined here, entail?
The most important observation is, that two equal objects are \emph{indiscernable}
by any attribute in the sense that for any type family $C : A \to \UU$ if
$C(a)$ and $a = b$, then we can prove $C(b)$, as we can prove in the following
lemma which defines an operation to achieve this:
\begin{lemma}[Transport]
Let $A : \UU$ and $C : A \to \UU$.
For any two elements $a, b : A$ with $p : a = b$ we can define the
\textbf{transport} operation on $p$:
\begin{equation*}
p^* : C(a) \to C(b)
\end{equation*}
\end{lemma}

\begin{proof}
By induction, we can assume $p$ to be $\refl_a$, on which we can define
$(\refl_a)^*(c) :\equiv c$.
\end{proof}

While it is obvious that equality should be an equivalence relation,
we only assume a witness for reflexivity.
But what about its symmetry and transitivity?
It turns out that these can be proven by induction without the need
to add them as constructors:
\begin{lemma}
Let again be $A : \UU$, $a, b : A$, and $p : a = b$.
Then, there is an equality proof $p\inv : b = a$, called the \textbf{inverse}
of $p$.
\end{lemma}

\begin{proof}
Applying the eliminator to $p$, we can reduce this problem to finding a path
$(\refl_a)\inv : a = a$, which we can define to be $\refl_a$ itself.
\end{proof}

\begin{lemma}
For $A : \UU$, elements $a, b, c : A$ and paths $p : a = b$ and $q : b = c$,
there is a path $p \ct q : a = c$, proving that propositional equality is
transitive.
\end{lemma}

\begin{proof}
Here it suffices to apply induction on the second path and give the definition
\begin{equation*}
p \ct \refl_b :\equiv p \text{.}
\end{equation*}
\end{proof}

Having provided the proof that equality is an equivalence relation,
we can already conclude that it makes each type a \emph{setoid}.
But more than that we can also prove that it carries the structure of a
\emph{groupoid}:
\begin{lemma}[Groupoid laws]
Let $A : \UU$, $a, b, c, d : A$ and $p : a = b$, $q : b = c$ and $r : c = d$.
Then,
\begin{itemize}
\item $p = p \ct \refl_b = \refl_a \ct p$,
\item $p\inv \ct p = \refl_b$, $p \ct p\inv = \refl_a$,
\item $(p\inv)\inv = p$ and
\item $p \ct (q \ct r) = (p \ct q) \ct r$.
\end{itemize}
\end{lemma}

\begin{proof}
The first three laws can be proven by induction on the path $p$, the last one
by induction on $r$.
\end{proof}

As a suitable notion of equality, propositional equality is respected by
functions:
\begin{lemma}
Let $A, B : \UU$, $f : A \to B$, and $a, b : A$. Then, there is a function
$\ap_f : (a = b) \to (f(a) = f(b))$ such that $\ap_f(\refl_a) \equiv \refl_{f(a)}$.
\hfill $\square$
\end{lemma}

Under this notion $\ap_f$, every function is functorial with respect to
equality.
Besides this, it is also functorial with respect to function composition.
The following laws can be proved by induction on the paths involved:
\begin{lemma}
Let $A, B, C : \UU$, $f : A \to B$ and $g : B \to C$. For paths $p : a =_A b$ and
$q : b =_A c$ we have equalities
\begin{itemize}
\item $\ap_f(p \ct q) = \ap_f(p) \ct \ap_f(q)$,
\item $\ap_f(p\inv) = \ap_f(p)\inv$,
\item $\ap_g(\ap_f(p)) = \ap_{g \circ f}(p)$, and
\item $\ap_{\id_A}(p) = p$. \hfill $\square$
\end{itemize}
\end{lemma}

If in the above considerations $f$ is instead a dependent function, we can not
necessarily consider the type $f(a) = f(b)$ since the left and right hand side
need not have the same type.
For this situation, there is a dependent version of the above defined function
$\ap_f$:
\begin{lemma}\label{thm:apd-hott}
Let $A : \UU$, $B : A \to \UU$ and $f : (a : A) B(a)$. Then, we can construct
a dependent function
\begin{equation*}
\apd_f : \{a, b\}(p : a =_A b) \to p_*(f(a)) =_{B(b)} f(b) \text{,}
\end{equation*}
such that $\apd_f(a, a, \refl_a) \equiv \refl_{f(a)}$. \hfill $\square$
\end{lemma}

We continue by considering one of the representation of one of the most import
notions of homotopy theory:
Homotopies between functions.
\begin{defn}[Homotopy of functions]\label{def:tt-htpy}
Two maps $f, g : A \to B$ are called \textbf{homotopic} or \textbf{pointwise equal}
if for all $a : A$ we have
$f(a) = g(a)$.
We define the notation
\begin{equation*}
f \sim g :\equiv (a : A) \to f(a) = g(a) \text{.}
\end{equation*}
We can define the same for two dependent functions $f, g : \prod_{(a : A)} B(a)$
over the same type family.
\end{defn}

\begin{remark}[Function Extensionality]\label{rmk:tt-funext}
Since in homotopy theory, we can find a path $p : f = g$ in the space of
functions, whenever $f \sim g$,
we might ask if in homotopy type theory this holds as well --
are two functions equal as soon as they are pointwise equal?
In the setting of homotopy type theory we will be able to derive this from
univalence, while in settings where we assume the K-rule, we will usually
assume this property of the function space, which is called \textbf{function extensionality},
axiomatically.
\end{remark}

So far, we have not considered what it means for two \emph{types to be equal}.
Of course, our definition of equality is valid for the universe itself as well,
but how does it relate to how we represent
biconditionals of propositions,
bijections between sets,
and homotopy equivalences between spaces?
One common representation of all of these three concepts is the notion of
equivalence of types:
\begin{defn}[Equivalences]\label{def:tt-hae}
Let $A, B : \UU$. A function $f : A \to B$ is called an \textbf{equivalence}
between $A$ and $B$, if there is a $g : B \to A$ such that
$\eta : g \circ f \sim \id_A$ and $\epsilon : f \circ g \sim \id_B$ and furthermore
\begin{equation*}
\tau : \prod_{a : A} \ap_f(\eta(a)) =_{f(g(f(a)))=a} \epsilon(f(a)) 
 \equiv \ap_f \circ \eta \sim \epsilon \circ f \text{.}
\end{equation*}
We need $\tau$ to make sure that each two witnesses for the fact that $f$ is an equivalence are
equal.
Since $\tau$ is one of the two commutativity conditions for pairs
of adjoint functors, this kind of equivalence is also called a \textbf{half
adjoint equivalence}.
%The type of witnesses for $f$ to be an equivalence shall be
%\begin{equation*}
%\isequiv(f) :\equiv \sum_{g : B \to A} ~ \sum_{\eta : g \circ f \sim \id_A} ~
%\sum_{\epsilon : f \circ g \sim \id_B} \ap_f \circ \eta \sim \epsilon \circ f \text{.}
%\end{equation*}
The type of all equivalences between two types $A, B : \UU$ is denoted by
\begin{equation*}
A \simeq B :\equiv \sum_{f : A \to B} \isequiv(f) \text{.}
\end{equation*}
\end{defn}
It is easy to show that the equivalence of types is indeed an equivalence relation
and that it behaves as we expect it to on easy examples such as
$(\unit \to A) \simeq A$, $(\emptytype + A) \simeq A$, and
$(\emptytype \times A) \simeq \emptytype$.

But how can we now compare the equivalence and equality on types?
By induction the reflexivity proof for equivalence is enough to show that
equality implies equivalence:
For each $A, B : \UU$ there is
\begin{equation*}
\idtoeqv_{A, B} : (A =_{\UU} B) \to (A \simeq B) \text{.}
\end{equation*}
But how about the other direction?
It turns out that there is no way to obtain equality from equivalence,
so one solution
is to assume axiomatically, that $\idtoeqv_{A, B}$ has an inverse function.
The consistency of this axiom has been shown by \cite{kapulkinlumsdaine},
after an idea by Vladimir Voevodsky.
\begin{axiom}[Univalence]
For every $A, B: \UU$ we asume that
$\idtoeqv_{A, B}$ is itself an equivalence.
This implies that
\begin{equation*}
(A =_\UU B) \simeq (A \simeq B)
\end{equation*}
and yields an inverse to $\idtoeqv_{A, B}$ which we call
\begin{equation*}
\ua_{A, B} : (A \simeq B) \to (A =_\UU B) \text{.}
\end{equation*}
\end{axiom}

\begin{remark}
Postulating axioms is avoided as often as possible in type theory, since it is detrimental
to one of the desirable properties of the type theory: normalization.
To give an example, with an axiom like $\ua$, not every closed term of the natural numbers
can be reduced to a numeral any more.
It is for this reason that type theorists have been looking for ways to achieve univalence
without postulating it as an axiom.
One approach is to not consider equality as an inductive type any more but to have it as
a primitive in the language.
This approach is used in a variety of type theory called ``cubical type theory''
\citep{cohen2016cubical}.
\end{remark}

Another instance of equalities we have not explored so far are \emph{iterated equalities}:
Equalities between equality proofs.
Since for types $A : \UU$ representing propositions we might only care about whether
or not we have a proof for it, it might not make sense to
consider multiple elements, so we might want that for all $a, b : A$ we have $a = b$.
For types that should represent sets we \emph{do} want multiple elements, but
we might not be interested in having different equality proofs --
so for $a, b : A$ and $p, q : a =_A b$ we might want to have a proof for $p =_{a = b} q$.
This effect, that at a certain level of iteration, all equality proofs should become
equal is called \textbf{truncation}:
\begin{defn}[$n$-types]
For a type $A : \UU$ we set
\begin{align*}
\isntype{(-1)}{(A)} \equiv \isProp(A) &:\equiv (a, b : A) \to a = b \text{,} \\
\isntype{0}{(A)} \equiv \isSet(A)  &:\equiv (a, b : A)(p, q : a = b) \to p = q \text{, and} \\
\isntype{(n + 1)}{(A)} &:\equiv (a, b : A) \to \isntype{n}{(a = b)} \text{ for $n : \N$.}
\end{align*}
and call $A$ a \textbf{(mere) proposition} if $\isProp(A)$, a \textbf{set} if $\isSet(A)$ and more
general an
\textbf{$n$-type} or $n$-truncated if $\isntype{n}{(A)}$.
We also extend this definition to include what it means for a type to represent a
contractible space, a singleton, or a true proposition:
\begin{equation*}
\isntype{(-2)}{(A)} \equiv \isContr(A) :\equiv (a : A) \times ((b : B) \to a = b) \text{.}
\end{equation*}
\end{defn}

There are some important, but easy to prove facts about truncatedness:
\begin{lemma}
\begin{itemize}
\item If $A : \UU$ is an $n$-type, then $A$ is an $(n+1)$-type.
\item For each $n \geq -2$ and $A : \UU$, the type $\isntype{n}(A)$ is a mere
proposition.
\item If $A : \UU$ is an $n$-type and $B : A \to \UU$ such that for each $a : A$,
$B(a)$ is an $n$-type, then $(a : A) \times B(a)$ is an $n$-type as well.
\item If $A : \UU$ and $B : A \to \UU$ are such that $B(a)$ is an $n$-type for each
$a : A$, then $(a : A) \to B(a)$ is an $n$-type as well.
\item Let $n \geq -1$. Then, $A : \UU$ is an $(n+1)$-type if and only if for all $a : A$,
the equality type $a =_A a$ is an $n$-type.
\newpage
\item Let $n \geq 0$. Then, $A : \UU$ is an $n$-type, if and only if for all $a : A$,
the \emph{$n$-fold iterated loop space} $\Omega^{n+1}(A, a)$ is contractible.
The $n$-fold iterated loop space is defined by recursion on $n$ by
\begin{align*}
\Omega^1(A, a) &:\equiv (a =_A a) \text{ and}\\
\Omega^{n+1}(A, a) &:\equiv (\refl_{\ddots_a} =_{\Omega^n(A,a)} \refl_{\ddots_a}) \text{.}
\end{align*}
\hfill $\square$
\end{itemize}
\end{lemma}

\begin{remark}[Uniqueness of Identity Proofs]\label{rmk:tt-uip}
Truncation levels are a notion which behaves quite differently
depending on whether we assume presence of the K-rule or not.

If, as in homotopy type theory, we assume univalence and only the J-rule,
it is an easy exercise to find types which are not sets.
In fact the universe $\UU$ itself is not a set \citepalias[Example 3.1.9]{hottbook}:
We can construct
two different automorphisms of $\twotype$: The identity function
and the map $f : \twotype \to \twotype$ with $f(0_\twotype) :\equiv 1_\twotype$
and $f(1_\twotype) :\equiv 0_\twotype$.
It is easy to prove that these are non-equal and as such, using univalence,
yield different instances of the type $\twotype = \twotype$.

If on the other hand we \emph{do} assume the K-rule, then it is immediate
-- by first applying the J-rule to $p$ and then the K-rule to $q$ -- that
for every type $A$ we can use the rule to show the inhabitedness of the
type family
\begin{equation*}
(a, b : A) (p, q : a = b) \to p = q \text{,}
\end{equation*}
and so $A$ is a set.
\end{remark}


\section{Theorem Provers Based on Type Theory}\label{sec:tt-provers}

As remarked at the beginning of \Cref{chp:tt}, we value type theory not only
for its capability to represent mathematics and logics, but also for its good
computational behaviour.
This suggests to have a look at how type theory is implemented in different languages
the focus of which can be either to be a functional programming language based
on dependent types or a theorem proving tool, or to be useful in both of these cases.
With regards to this thesis, introducing different implementations is relevant
in two ways:
\begin{enumerate}
\item A good part of the new results in the form of definitions, lemmas,
and theorems, has been formalized in a one of these languages, and
\item both the higher Seifert-van Kampen theorem presented in~\Cref{sec:paths-svk}
as well as the attempted reduction from inductive-inductive type to inductive
families in ~\Cref{chp:red} suggest useful new features or extensions of implementations
of type theory.
\end{enumerate}
We will thus concentrate on the two languages we have used in this respect:
Lean~\citep{mouracade} and Agda~\citep{agda}.
This does not mean that the content discussed is irrelevant to other implementations
or that these are less important.
Au contraire -- the theorem prover Coq~\citep{coq} is not only the implementation
with the biggest user base but also has a lot of distinguishing features.

Before we delve into the differences between Lean and Agda, we will first
take a look at what these provers have \emph{in common}.
The following list contains both fundamental design principles as well as
several features which are important to improve the usability of the
implementation:
\begin{itemize}
\item Implementation of \emph{Martin-Löf type theory} with an unlimited chain of universes.
Construction can be made \emph{universe polymorphic} by the use of
universe variables which are used as variable indices for the unvierses.
\item Function types distinguish between explicit and \emph{implicit arguments}.
The latter are useful to make definitions more succinct.
\item Both languages are equipped with a system of \emph{modules} or \emph{name spaces}
used to organize definitions and their scope.
\item There is a way to customize the implementation by the means of \emph{options}.
This enables us to switch between a set-truncated setting and
homotopy type theory.
\item The implementations have an source code which is open and thus reviewable and
customizable by everybody.
\item There are a one or several text editors which support an interactive
and well-integrated use of the language.
\end{itemize}

Let us next concentrate on Lean as our first specimen.

\subsection{Lean}\label{sec:tt-lean}

The development of the theorem prover Lean was started by
Leonardo de Moura at Microsoft Research in 2013.
It aims to be both a useful tool for the formalization of mathematics and
the verification of programs written in other languages, as well as to
be a useful and performant programming language itself.
Another goal of Lean is to make automated theorem proving (such as the solution
for SMT-style problems) available in a dependently typed theorem prover.
As of now, Lean is in its third major version, after a lot of the system was
overhauled in the transition from Lean 2.
The fourth version is under development but not yet ready to be used as a prover.

Lean -- as its name suggests -- relies on a relatively small trusted kernel to which
the user code is compiled to.
This makes it easier to make claims about the consistency of the prover.
Let us first take a look at some of the specifics of Lean's type theory.

Lean is based on \emph{inductive families} as described by \citet{dybjer94}.
Whenever the user writes down the definition of such an inductive family,
Lean generates its constructors and its dependent eliminator and makes
it available to the user.
For example, the following snippet shows a definition of the natural numbers:
\begin{leancodebr}
inductive nat : Type 0
| zero : nat
| succ : nat → nat

#check nat.succ nat.zero -- Prints nat.succ nat.zero : nat
#check @nat.rec_on -- Π {C : nat → Type u} (n : nat),
                   --   C nat.zero →
                   --   (Π (a : nat), C a → C (nat.succ a)) → C n
\end{leancodebr}

A widely used subclass of inductive types are \textbf{structures}, which are
similar to what is often referred to as ``records''.
Structures are inductive types which are non-recursive and only have one constructor.
That means that they are equivalent to iterated sigma types but, among other advantages,
have named projections.
Structures provide a basic inheritance mechanism as they can extend other structures:
\begin{leancode}
structure graph (V : Type) :=
  (E : V → V → Type)

structure refl_graph (V : Type) extends graph V :=
  (refl : Π (v : V), E v v)

structure trans_graph (V : Type) extends graph V :=
  (trans : Π (u v w : V), E u v → E v w → E u w)
\end{leancode}

Lean features a strict and impredicative universe \leani{Prop} of propositions.
Since this universe is incompatible with homotopy type theory, we have to be
careful to exclude its use whenever we want formalizations to hold in
a setting of homotopy type theory instead of truncated type theory.
\emph{Strict} here means that if we have a proposition \leani{p : Prop} and two
proofs \leani{a, b : P}, these are considered judgmentally equal, thus for
every type family \leani{P → Type}, the fibers \leani{P a} and \leani{P b}
are judgmentally equal as well.
\emph{Impredicative} means, that for every type family valued in \leani{Prop},
its universal quantification \leani{∀ a, P a} is in \leani{Prop} as well, even
if \leani{A} is not.

Lean makes heavy use of \emph{tactics}.
These are commands which can modify a given proof goal or a series of proof
goals.
They are accesible once the user has switched from a mode where she can
enter literal proof terms to a \emph{tactic-mode},
which allows her to give proofs and definitions as a sequence of tactic applications,
which are then desugared as the do-notation of a \emph{tactic monad}.
Here is an example for a tactic proof in lean, using a simplification tactic,
a rewriting tactic, and a tactic calling previously defined theorems:
\begin{leancodebr}
example : ∀ (n : ℕ), n = 0 ∨ n > 0 :=
begin
  intro n,
  induction n with n' IH,
  simp,
  right,
  cases IH,
  rw IH, constructor,
  apply nat.zero_lt_succ
end
\end{leancodebr}

Lean has its own \emph{meta-language} which allows the user to define new
language features like commands and tactics.
It does so by providing a way to reflect arbitrary terms into an inductive \emph{type of
expressions} \leani{expr},
such that by recursion on this type, structural recursion on the term can 
be emulated.

Lean has several big libraries, the biggest one, which provides basic data types,
tactics, and formalization in different fields of mathematics, is called
\emph{Mathlib} (\url{https://github.com/leanprover-community/mathlib}).
It is not recommended to use Lean without using Mathlib.
Another big formalization effort it Lean's homotopy type theory library,
the development of which has been described by \citet{leanhott}.
It was originally written in Lean 2 and then ported to Lean 3.
A good introduction to the language can be found online \citep{avigad2015theorem}.

\subsection{Agda}

Agda is a theorem prover. Its first version was written by Catarina Coquand, while
the development of the current major version, Agda 2, was initiated by
Andreas Abel and Ulf Norell in 2005.
There are biannual ``Agda Implementors Meetings'' to discuss new ideas on how to
improve Agda.

Compared to Lean, Agda implements a wider range of inductive types,
also allowing \emph{inductive-inductive types}.
In contrast to Lean it does not automatically produce the eliminator for an
inductive definition but instead provides an induction principle based
on \emph{dependent pattern matching}:
The user can write (dependent) functions with an inductive domain, by pattern
matching on its input.
In the following example, an indexed type of vectors and its head function are
defined.
Agda's pattern matching algorithm recognizes that it can exclude the case of the first
constructor in the definition of the head function.
\begin{agdacode}
data Vec (A : Set) : N → Set where
  nil : Vec A Z
  cons : ∀{n} → A → Vec A n → Vec A (S n)

hd : ∀{A n} → Vec A (S n) → A
hd (cons a v) = a
\end{agdacode}

Agda has several options which make it possible to use it with a range of different
type theories:
Homotopy type theory is supported by declaring via an option \agdai{--without-K}
that it should reject any use of the K-rule.
Agda also has an option \agdai{--rewriting} whic makes it possible to declare
many equalities as strict by declaring their proofs to be \emph{rewrite rules}
\citep{cockxrewrite}, giving the type theory the flavour of extensional type
theory.

Agda has also been modified to support a type theory which provides
a constructive, non-axiomatic version of univalence.
Based on the geometry of its identity types, this type theory is called
\emph{cubical type theory}, and the modified version has the name
``Cubical Agda'' \citep{andrea:cubicalagda}.

%TODO
% ** ttintt
% 








